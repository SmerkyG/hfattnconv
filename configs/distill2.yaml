model_path: ./out1
train:
  attention_distillation_stage: 2
  output_dir: out2
  sequence_length: 512
  train_dataset_path: robbiegwaldd/dclm-10B
  token_count: 40000000
  training_args:
    output_dir: out
    bf16: True
    per_device_train_batch_size: 4
    gradient_checkpointing: False
    learning_rate: 3e-4
    adam_beta1: 0.9
    adam_beta2: 0.95
    lr_scheduler_type: cosine
    warmup_steps: 50
    save_total_limit: 1
    include_tokens_per_second: True
    logging_steps: 10
    dataloader_num_workers: 8
