train:
  wandb: huggingface

  attention_distillation_stage: 2
  output_dir: out2
  sequence_length: 512
  train_dataset_path: robbiegwaldd/dclm-10B
  token_count: 100000000

  per_device_train_batch_size: 4
  gradient_checkpointing: False
  lr_decay_type: cosine
  lr_init: 1e-3
  lr_final: 1e-8
  warmup_steps: 50
  beta1: 0.9
  beta2: 0.95
  #save_total_limit: 1
  log_every_n_steps: 10
  dataloader_num_workers: 2
  #save_steps: 2000

  devices: 8

  strategy: deepspeed_stage_1
