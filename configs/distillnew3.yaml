train:
  wandb: huggingface

  attention_distillation_stage: 3
  output_dir: out3
  sequence_length: 512
  train_dataset_path: robbiegwaldd/dclm-10B
  token_count: 100000000

  per_device_train_batch_size: 12
  gradient_checkpointing: True
  lr_decay_type: linear
  lr_init: 1e-5
  lr_final: 1e-5
  warmup_steps: 50
  beta1: 0.9
  beta2: 0.95
  #save_total_limit: 1
  log_every_n_steps: 10
  dataloader_num_workers: 2
  #save_steps: 2000

  devices: 8

  strategy: deepspeed_stage_1
